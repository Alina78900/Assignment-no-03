# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ICsyvUHIajm3WSofUVrMfkd61QcQ2rsK
"""

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Install required packages
!pip install pandas transformers datasets sentencepiece --quiet

# Step 2: Clone the dataset from GitHub
!git clone https://github.com/akabircs/BeliN.git
# %cd BeliN/Dataset

# Step 3: List the dataset files
!ls

# Step 4: Load the data
import pandas as pd

train_df = pd.read_csv("/content/BeliN/Dataset/train.csv")
# Change pd.read_csv to pd.read_excel for .xlsx file
val_df = pd.read_excel("/content/BeliN/Dataset/Dataset.xlsx")
test_df = pd.read_csv("/content/BeliN/Dataset/test.csv")

# Rename columns in train_df, val_df, and test_df to match the expected format in preprocess_data
# Rename columns for train_df
train_df = train_df.rename(columns={
    'Category': 'category',
    'Article': 'article',
    'Aspect': 'aspect',
    'Sentiment': 'sentiment',
    'Headlines': 'headline'
})

# Rename columns for val_df (already exists in original code)
val_df = val_df.rename(columns={
    'Category': 'category',
    'Article': 'article',
    'Aspect': 'aspect',
    'Sentiment': 'sentiment',
    'Headlines': 'headline'
})

# Rename columns for test_df (assuming it will be preprocessed later)
test_df = test_df.rename(columns={
    'Category': 'category',
    'Article': 'article',
    'Aspect': 'aspect',
    'Sentiment': 'sentiment',
    'Headlines': 'headline'
})


print("Train Data Sample:")
print(train_df.head())

print("Train Data Columns:", train_df.columns.tolist())
print("Validation Data Columns:", val_df.columns.tolist()) # Print validation data columns
print("Test Data Columns:", test_df.columns.tolist())     # Print test data columns


# Step 5: Preprocess the data for headline generation
from datasets import Dataset

def preprocess_data(df):
    # Check if the expected columns exist before accessing them
    expected_columns = ['category', 'aspect', 'sentiment', 'article', 'headline']
    if not all(col in df.columns for col in expected_columns):
        missing_cols = [col for col in expected_columns if col not in df.columns]
        raise ValueError(f"DataFrame is missing required columns: {missing_cols}. Available columns: {df.columns.tolist()}")

    inputs = df['category'] + ' [SEP] ' + df['aspect'] + ' [SEP] ' + df['sentiment'] + ' [SEP] ' + df['article']
    targets = df['headline']
    return pd.DataFrame({'input_text': inputs, 'target_text': targets})

train_data = preprocess_data(train_df)
val_data = preprocess_data(val_df)

train_dataset = Dataset.from_pandas(train_data)
val_dataset = Dataset.from_pandas(val_data)

print("\nSample Preprocessed Data:")
print(train_data.head())

# Step 6: Load tokenizer and model (mT5 or Bengali T5)
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "google/mt5-small"  # You can replace this with a Bengali fine-tuned T5 if available
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Step 7: Tokenize the dataset
def tokenize_function(batch):
    model_inputs = tokenizer(batch['input_text'], max_length=512, truncation=True)
    labels = tokenizer(batch['target_text'], max_length=64, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val = val_dataset.map(tokenize_function, batched=True)

# Step 7: Tokenize the dataset
def tokenize_function(batch):
    model_inputs = tokenizer(batch['input_text'], max_length=512, truncation=True, padding="max_length") # Add padding here
    labels = tokenizer(batch['target_text'], max_length=64, truncation=True, padding="max_length") # Add padding here
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val = val_dataset.map(tokenize_function, batched=True)

# %%
# Step 8: Set training arguments and Trainer
from transformers import TrainingArguments, Trainer, DataCollatorWithPadding

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
)

# Initialize DataCollatorWithPadding
# It's crucial that the tokenizer used here is the same one used for tokenization
# Since we are padding in the tokenize_function, the data collator
# can still be used, but the primary padding is handled earlier.
# The DataCollatorWithPadding will still ensure all features are in the correct format.
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True) # Explicitly set padding to True

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    data_collator=data_collator, # Add the data collator here
)

# Step 9: Train the model
trainer.train()

# Optional: Save the model
trainer.save_model("bengali_headline_gen_model")

print("Training Complete!")

# Step 10: Load the trained model (optional if already in memory)
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import os

# Load from the path where the full model and tokenizer were saved
model_path = "/content/BeliN/Dataset/bengali_headline_gen_model"

# Verify the contents of the directory
print(f"\nContents of {model_path}:")
if os.path.exists(model_path):
    !ls {model_path}
else:
    print(f"Directory {model_path} does not exist.")


# Now attempt to load the tokenizer and model
try:
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    print("\nModel and Tokenizer loaded successfully!")

    # Example: Generate headline for one article from the test set
    sample = test_df.iloc[0]

    # Prepare input as done in training
    input_text = sample['category'] + ' [SEP] ' + sample['aspect'] + ' [SEP] ' + sample['sentiment'] + ' [SEP] ' + sample['article']
    print("\nInput Text:")
    print(input_text)

    # Tokenize input
    inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)

    # Generate headline
    # Ensure the model is in evaluation mode
    model.eval()
    # Use the correct device for generation
    import torch
    if torch.cuda.is_available():
        model.to('cuda')
        inputs = {k: v.to('cuda') for k, v in inputs.items()}


    output = model.generate(**inputs, max_length=64, num_beams=4, early_stopping=True)
    generated_headline = tokenizer.decode(output[0], skip_special_tokens=True)

    print("\nGenerated Headline:")
    print(generated_headline)

    # Actual headline for comparison
    print("\nActual Headline:")
    print(sample['headline'])

except Exception as e:
    print(f"\nError loading model or generating headline: {e}")
    print("Please check the contents of the results directory and the saving process.")